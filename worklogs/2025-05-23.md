# Worklog for 2025-05-23

## Summary of Activities
- Revise webserver role to retrieve client IP
- Develop test script to manually test loadbalancer requirements
- Revised loadbalancer role with stick table


## Process

START TIME: 3:33 PM CST

I believe I have identified the problem.  While I am passing the client's IP address in the HAProxy config with the
`option forwardfor` line, I didn't modify the nginx config to read that value and retrieve the client's IP address.
To quote Homer Simpson, "D'oh".  I'll modify the webserver config to read that header and hopefully that should 
resolve the issue. I can dynamically add the line to the file but may be easier to just create a template file
of the config and copy it over to ensure the config is exactly what we expect it to be.  Going to work on that now.

Success! I'm now seeing my client IP in the access logs. Had to make a small refactor to the webserver role to 
accomplish this.  I pulled down the default `nginx.conf` that was on the webserver and added the below lines to it.
This allowed for the client IP to show up in the access logs. (also seen below) 

nginx.conf addition
```
# Allow real Client IP to be passed from loadbalancer
	set_real_ip_from {{ hostvars['lb']['ansible_host'] }};
	real_ip_header X-Forwarded-For;
```

Access Logs snippet:
```
34.220.9.184 - - [24/May/2025:04:05:10 +0000] "GET /index.html HTTP/1.0" 200 133 "-" "-"
24.162.14.29 - - [24/May/2025:04:05:11 +0000] "GET / HTTP/1.1" 200 133 "-" "curl/8.7.1"
34.220.9.184 - - [24/May/2025:04:05:12 +0000] "GET /index.html HTTP/1.0" 200 133 "-" "-"
24.162.14.29 - - [24/May/2025:04:05:12 +0000] "GET / HTTP/1.1" 200 133 "-" "curl/8.7.1"
24.162.14.29 - - [24/May/2025:04:05:14 +0000] "GET / HTTP/1.1" 200 133 "-" "curl/8.7.1"
34.220.9.184 - - [24/May/2025:04:05:14 +0000] "GET /index.html HTTP/1.0" 200 133 "-" "-"
24.162.14.29 - - [24/May/2025:04:05:15 +0000] "GET / HTTP/1.1" 200 133 "-" "curl/8.7.1"
24.162.14.29 - - [24/May/2025:04:05:16 +0000] "GET / HTTP/1.1" 200 133 "-" "curl/8.7.1"
34.220.9.184 - - [24/May/2025:04:05:16 +0000] "GET /index.html HTTP/1.0" 200 133 "-" "-"
24.162.14.29 - - [24/May/2025:04:05:17 +0000] "GET / HTTP/1.1" 200 133 "-" "curl/8.7.1"
34.220.9.184 - - [24/May/2025:04:05:18 +0000] "GET /index.html HTTP/1.0" 200 133 "-" "-"
```
Note that my client IP is `24.162.14.29` while `34.220.9.184` is the haproxy IP.

In order to verify that my configuration accomplished all that was requested of me, I felt it was a good idea to
write a test script that I can run locally for sanity checks.  I wanted it to accomplish the main 4 things: port
range 60000-65000 and 80 all get fed to the webservers on port 80, Verify "stickiness", pass the requesting IP to
the webservers, and ensure that a switch is done only when a webserver goes down and doesn't switch back when it
comes back online.  Going to work on this now.

Script is completed and I'm glad I developed it as I caught a case that my configuration failed at.  While the
haproxy was indeed sticky and hit the same server and successfully failed over to the other webserver, it returned
the the previous server when it came back online.  This is not good and doesn't satisfy the request. Going to 
research this and reconfigure.

So I'm going to need to rethink my routing algorithm.  `balance source` uses a hash of the source IP to "stick"
clients to a server. However, when a server goes down and comes back up, new connections from the same IP may 
hash back to the original server which is the behavior I was observing when I ran my test script. To fix, I'm
going to need to initailize a stick table and set `option persist` for the backend configuration. This should
resolve the issue.

Eureka! That did the trick!  My configuration initially routes me to server `B` until I stop the nginx service on
that server, at which point it will route me to server `A` even when server `B` comes back online.  I think I'm 
comfortable to say that the loadbalancer config is officially done. Next will be the nagios monitoring but that 
will be for a later date. 

END TIME: 7:22 PM CST


## Next Steps
- Design/code Nagios monitoring configuration role
- Create better architecture diagram for README (lower priority)
- Refine work notes (add missing pictures and clean up grammar) (lower priority)